{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.51.2-cp36-cp36m-manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 95 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba) (50.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from numba) (1.19.4)\n",
      "Collecting llvmlite<0.35,>=0.34.0.dev0\n",
      "  Downloading llvmlite-0.34.0-cp36-cp36m-manylinux2010_x86_64.whl (24.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.6 MB 9.4 MB/s eta 0:00:01    |██████▍                         | 4.9 MB 100 kB/s eta 0:03:16     |██████████████▊                 | 11.3 MB 6.0 MB/s eta 0:00:03\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.34.0 numba-0.51.2\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.3-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 34.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.19.4)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Installing collected packages: kiwisolver, pillow, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.3 pillow-8.0.1\n",
      "Collecting ray\n",
      "  Downloading ray-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (23.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.1 MB 11.2 MB/s eta 0:00:01     |████████████████████████████    | 20.2 MB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencensus\n",
      "  Downloading opencensus-0.7.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gpustat\n",
      "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting aioredis\n",
      "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 19.8 MB/s eta 0:00:01     |█████████████████████████▊      | 2.3 MB 19.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click>=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting redis<3.5.0,>=3.3.2\n",
      "  Downloading redis-3.4.1-py2.py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting colorful\n",
      "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.7.2-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray) (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray) (1.19.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray) (2.25.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray) (3.2.0)\n",
      "Collecting google\n",
      "  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (274 kB)\n",
      "\u001b[K     |████████████████████████████████| 274 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray) (1.32.0)\n",
      "Collecting aiohttp-cors\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray) (0.8.0)\n",
      "Collecting google-api-core<2.0.0,>=1.0.0\n",
      "  Downloading google_api_core-1.23.0-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting opencensus-context==0.1.2\n",
      "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (1.15.0)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.7.3.tar.gz (465 kB)\n",
      "\u001b[K     |████████████████████████████████| 465 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blessings>=1.6\n",
      "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
      "Collecting hiredis\n",
      "  Downloading hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 5.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting async-timeout\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (20.3.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.0.4)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.0.0-cp36-cp36m-manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.2-cp36-cp36m-manylinux2014_x86_64.whl (295 kB)\n",
      "\u001b[K     |████████████████████████████████| 295 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray) (50.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->ray) (2.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema->ray) (2.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema->ray) (0.17.3)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.23.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 5.3 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pytz\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->ray) (3.4.0)\n",
      "Collecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Downloading soupsieve-2.0.1-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.1.1)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 4.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n",
      "Building wheels for collected packages: gpustat, pyyaml, nvidia-ml-py3, psutil, idna-ssl, contextvars\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=1b5b660e14d08bcd942610955dfa408db70ef932c42d137cb917ae24b3a7a360\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/da/35/fe2cfb3bc47822299f5e124a599d56f00b30ec0b328db16b9f\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=f79e2eef7d7842ee268e6ec269679a66463468104c6e86af63bdb0ef6bb48dcd\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=b02a3874b28de6269654a401f967a9e8b6a0f3102e4fa38fab77fab0cad9d600\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/26/a3/33f2079871e2bebb3f53a2b21c3ec64129b8efdd18a6263a52\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.3-cp36-cp36m-linux_x86_64.whl size=281527 sha256=2eb354b2794b3f455cb4924cbea56af66021966895f2b8a59d62854d546f9ec0\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/ad/67/90bbaacdcfe970960dd5158397f23a6579b51d853720d7856d\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=7be1625dfa067d77f4a12e819d0f32125c86ac707593a6532322118bf28e43b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7665 sha256=e73acb0171a61173b9d900fa50c2dcf87c001aeec71407f6ebd62fc7907897c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/11/53/911724983aa48deb94792432e14e518447212dd6c5477d49d3\n",
      "Successfully built gpustat pyyaml nvidia-ml-py3 psutil idna-ssl contextvars\n",
      "Installing collected packages: googleapis-common-protos, pytz, google-api-core, immutables, contextvars, opencensus-context, opencensus, nvidia-ml-py3, psutil, blessings, gpustat, hiredis, async-timeout, aioredis, py-spy, click, redis, colorful, idna-ssl, multidict, yarl, aiohttp, colorama, filelock, soupsieve, beautifulsoup4, google, pyyaml, msgpack, aiohttp-cors, ray\n",
      "Successfully installed aiohttp-3.7.2 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 beautifulsoup4-4.9.3 blessings-1.7 click-7.1.2 colorama-0.4.4 colorful-0.5.4 contextvars-2.4 filelock-3.0.12 google-3.0.0 google-api-core-1.23.0 googleapis-common-protos-1.52.0 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 msgpack-1.0.0 multidict-5.0.0 nvidia-ml-py3-7.352.0 opencensus-0.7.11 opencensus-context-0.1.2 psutil-5.7.3 py-spy-0.3.3 pytz-2020.4 pyyaml-5.3.1 ray-1.0.1 redis-3.4.1 soupsieve-2.0.1 yarl-1.6.2\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.51.0\n",
      "Collecting more_itertools\n",
      "  Downloading more_itertools-8.6.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.0 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: more-itertools\n",
      "Successfully installed more-itertools-8.6.0\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.19.4)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 48.4 MB/s eta 0:00:01    |██████▊                         | 5.5 MB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=0b70402463c215bb8d796f2fe27b87fec004444b6c568b0511e4cd0c48a01b93\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, threadpoolctl, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.17.0 scikit-learn-0.23.2 scipy-1.5.4 sklearn-0.0 threadpoolctl-2.1.0\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.7)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numba\n",
    "!pip install matplotlib\n",
    "!pip install ray\n",
    "!pip install tqdm\n",
    "!pip install more_itertools\n",
    "!pip install sklearn\n",
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "import more_itertools\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "from numba import jit ,f8\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sklearn\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver & Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def generate_grid(n,bc,ic=0):\n",
    "    #n=> Number of interior nodes\n",
    "\n",
    "    A = np.ones((n+2,n+2),dtype=np.float32) * ic\n",
    "    A[0,0]=A[-1,-1]=A[0,-1]=A[-1,0]=0\n",
    "    A[0,1:-1]=bc[2]    # switch the top and bottom wall since we start the iterations from top\n",
    "    A[1:-1,-1]=bc[1]\n",
    "    A[-1,1:-1]=bc[0]\n",
    "    A[1:-1,0]=bc[3]\n",
    "\n",
    "    return A\n",
    "\n",
    "@jit(nopython=True)\n",
    "def update_grid_col(grid,x,n):\n",
    "    #first half step update\n",
    "    #x col values ;\n",
    "    #n col number \n",
    "    grid[1:-1,n] = x\n",
    "    return grid\n",
    "\n",
    "@jit(nopython=True)\n",
    "def update_grid_row(grid,x,n):\n",
    "    #second half step update\n",
    "    #x row values ;\n",
    "    #n col number \n",
    "    grid[n,1:-1] = x\n",
    "    return grid\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_first_half(A,i,j,lam):\n",
    "    #calculate the ADI explicit part of the equation first half step\n",
    "    if j==1 :\n",
    "        return  lam * A[j][i-1] + 2 *(1-lam) * A[j][i] + lam * A[j][i+1] + lam * A[j-1][i]\n",
    "    elif j== A.shape[0]-2 :\n",
    "        return  lam * A[j][i-1] + 2 *(1-lam) * A[j][i] + lam * A[j][i+1] + lam * A[j+1][i]\n",
    "    else  :\n",
    "        return  lam * A[j][i-1] + 2 *(1-lam) * A[j][i] + lam * A[j][i+1]\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_second_half(A,i,j,lam):\n",
    "    #calculate the ADI explicit part of the equation second half step\n",
    "    if i==1 :\n",
    "        return  lam * A[j-1][i] + 2 *(1-lam) * A[j][i] + lam * A[j+1][i] + lam * A[j][i-1]\n",
    "    elif i== A.shape[0]-2 :\n",
    "        return  lam * A[j-1][i] + 2 *(1-lam) * A[j][i] + lam * A[j+1][i] + lam * A[j][i+1]\n",
    "    else  :\n",
    "        return  lam * A[j-1][i] + 2 *(1-lam) * A[j][i] + lam * A[j+1][i]\n",
    "\n",
    "@jit(nopython=True)\n",
    "def generate_TDM(Lambda,N=3):\n",
    "    a = np.ones(N-1,dtype=np.float32)*-Lambda \n",
    "    b =np.ones(N,dtype=np.float32) * 2*(Lambda+1) \n",
    "    return a,b,a\n",
    "\n",
    "@jit(f8[:](f8[:],f8[:],f8[:],f8[:]))\n",
    "def TDMA_solver(a0,b0,c0,d0):\n",
    "    a =np.copy(a0)\n",
    "    b =np.copy(b0)\n",
    "    c =np.copy(c0)\n",
    "    d =np.copy(d0)\n",
    "    ld = len(d)\n",
    "\n",
    "    for i in range(1,ld):\n",
    "        w    = a[i-1]/b[i-1]\n",
    "        b[i] = b[i]- w * c[i-1]\n",
    "        d[i] = d[i] -w * d[i-1]\n",
    "    \n",
    "    R=b\n",
    "    R[-1]=d[-1]/b[-1]\n",
    "    \n",
    "    for i in range(ld-2,-1,-1):\n",
    "        R[i]= (d[i]-c[i]*R[i+1]) /b[i]\n",
    "        \n",
    "    return R\n",
    "\n",
    "@jit(nopython=True)\n",
    "def ADI_first_half_step(grid,Lambda,a,b,c):\n",
    "    #apply ADI for single step\n",
    "\n",
    "    N = grid.shape[0]\n",
    "\n",
    "    ##First half step\n",
    "    for i in range(1,N-1):\n",
    "\n",
    "        # initialize explicit side of equation to zeros\n",
    "        d = np.zeros((N-2))\n",
    "\n",
    "        #move vertically implcitly and calculate horizontally explicitly\n",
    "        for j in range(1,N-1):\n",
    "            d[j-1] =calculate_first_half(grid,i,j,Lambda)\n",
    "\n",
    "        x = TDMA_solver(a,b,c,d)\n",
    "\n",
    "        grid = update_grid_col(grid,x,i)\n",
    "\n",
    "    return grid\n",
    "\n",
    "@jit(nopython=True)\n",
    "def ADI_second_half_step(grid,Lambda,a,b,c):\n",
    "    #apply ADI for single step\n",
    "\n",
    "    N = grid.shape[1]\n",
    "\n",
    "    ##Second half step\n",
    "    for i in range(1,N-1):\n",
    "\n",
    "        # initialize explicit side of equation to zeros\n",
    "        d = np.zeros((N-2))\n",
    "\n",
    "        #move horizontally implcitly and calculate vertically explicitly\n",
    "        for j in range(1,N-1):\n",
    "            d[j-1] =calculate_second_half(grid,j,i,Lambda)\n",
    "\n",
    "        x = TDMA_solver(a,b,c,d)\n",
    "\n",
    "        grid = update_grid_row(grid,x,i)\n",
    "\n",
    "    return grid\n",
    "\n",
    "@jit(nopython=True)\n",
    "def solve(grid,Lambda=1,iters=1,steps=False) :\n",
    "    if steps :\n",
    "        #save intermeidate steps\n",
    "\n",
    "        #(frames,height,width)\n",
    "        grids = np.zeros((iters+1, grid.shape[0],grid.shape[1]),dtype=np.float32)\n",
    "        grids[0,:,:]=grid\n",
    "        a,b,c = generate_TDM(Lambda,grid.shape[0]-2)\n",
    "        #apply ADI iteratively\n",
    "        for i in range(1,iters+1):\n",
    "            grids[i,:,:] = ADI_first_half_step(grid,Lambda,a,b,c)\n",
    "            grids[i,:,:] = ADI_second_half_step(grid,Lambda,a,b,c)\n",
    "\n",
    "\n",
    "    else : \n",
    "        grids = np.zeros((2,grid.shape[0],grid.shape[1]),dtype=np.float32)\n",
    "        #Show final step only\n",
    "        grids[0,:,:]=grid\n",
    "        a,b,c = generate_TDM(Lambda,grid.shape[0]-2)\n",
    "        #apply ADI iteratively\n",
    "        for i in range(iters):\n",
    "            grids[1,:,:] = ADI_first_half_step(grid,Lambda,a,b,c)\n",
    "            grids[1,:,:] = ADI_second_half_step(grid,Lambda,a,b,c)\n",
    "\n",
    "    return grids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_grids(grids,Lambda):\n",
    "    grids[:,0,0]=grids[:,-1,0]=grids[:,0,-1]=grids[:,-1,-1]=Lambda * 1000\n",
    "    return grids\n",
    "\n",
    "@ray.remote\n",
    "def solve_permutation(n,iters,permutation):\n",
    "    Lambda,bc1,bc2,bc3,bc4,ic = permutation\n",
    "    grid = generate_grid(n-2,bc=(bc1,bc2,bc3,bc4),ic=ic)\n",
    "    ADIsoltuion = solve(grid.copy(),Lambda = Lambda,iters =iters ,steps=True)\n",
    "    return  np.array(pad_grids(ADIsoltuion,Lambda)).reshape(ADIsoltuion.shape[0],ADIsoltuion.shape[1],ADIsoltuion.shape[2],1)\n",
    "\n",
    "\n",
    "def generate_data(N,iters,permutations):\n",
    "    '''\n",
    "    Input : \n",
    "    N            : size of grid\n",
    "    iters        : max iterations done by solver\n",
    "    permutations : the solution parameters as a set of permutation (Lambda,bc1,..bc4,ic0)\n",
    "    \n",
    "    Output:\n",
    "    solution with shape (iters+1,N,N)\n",
    "    '''\n",
    "    data = [(solve_permutation.remote(N,iters,i)) for i in (permutations) ]\n",
    "    return np.array([ray.get(datalet) for datalet in (data)])\n",
    "    \n",
    "\n",
    "def generate_data_random_permutations(lR=(0,0.25),tR=(0,1000),batches =1,batch_size=32,seed =42,split=1 ):\n",
    "    '''\n",
    "    Input:\n",
    "    *Lambda range\n",
    "    *Temperature range\n",
    "    *Size of data\n",
    "    \n",
    "    Output:\n",
    "    *Generate a generator of size with elements of (Lambda,bc1,..bc4,ic0)\n",
    "    '''\n",
    "    np.random.seed(seed);\n",
    "    lr = np.random.randint(low = tR[0] , high = tR[1] ,size=(batches,batch_size,6)).astype('float')\n",
    "    lr[:,:,0] = ((lR[1]-lR[0])*(lr[:,:,0]-tR[0]))/(tR[1]-tR[0])\n",
    "    return lr\n",
    "\n",
    "\n",
    "def generate_data_batches(N=50,\n",
    "                          lR=(0,0.5),\n",
    "                          tR=(0,100),\n",
    "                          max_iters=10,\n",
    "                          seed=42,\n",
    "                          steps=1,\n",
    "                          step_size=1,\n",
    "                          batch_size=32,\n",
    "                          batches=100,\n",
    "                          progress=True,\n",
    "                          key_bias =0,\n",
    "                          save_file = None):\n",
    "    '''\n",
    "    return dictionary with key of the batch number\n",
    "    '''\n",
    "    if save_file is not None : \n",
    "        hf = h5py.File(save_file,'w')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    perms = generate_data_random_permutations(lR=lR,tR=tR,batch_size=batch_size,batches=batches,seed=seed)\n",
    "    iters_list=  np.random.randint(low=step_size,high=max_iters-step_size*(steps-1)+1,size=batches)\n",
    "\n",
    "    #scaling \n",
    "    mean = (tR[1]-tR[0])/2\n",
    "    std  = mean/2\n",
    "    data={}\n",
    "    \n",
    "    for batch_num in tqdm(range(batches)):\n",
    "        \n",
    "        iter_n = iters_list[batch_num]\n",
    "        \n",
    "        generated_data = generate_data(N,iter_n+steps*step_size,perms[batch_num])\n",
    "        extract_index = np.arange(iter_n-step_size,iter_n+step_size*steps,step_size)\n",
    "#         yield(extract_index)\n",
    "        generated_data = generated_data[:,extract_index,:,:,:]\n",
    "        \n",
    "        if save_file is None :\n",
    "            data[f'{batch_num + key_bias}'] = (generated_data -mean) /std\n",
    "        else : \n",
    "            hf.create_dataset(f'{batch_num}',data = data[batch_num] , compression ='gzip')\n",
    "    \n",
    "    if save_file is not None :hf.close()\n",
    "    else : return data\n",
    "\n",
    "    \n",
    "def data_generator(data):\n",
    "    '''\n",
    "    input : data dictionary (batch number :5D tensor data)\n",
    "    output: input , target values\n",
    "    '''\n",
    "    batches = len(data.keys())\n",
    "    batch_size = len(data['0'])\n",
    "    batch_counter= 0\n",
    "    \n",
    "    while True:\n",
    "        x,y = data[f'{batch_counter}'][:,:-1,:,:,:],data[f'{batch_counter}'][:,-1:,:,:,:]\n",
    "\n",
    "        batch_counter += 1\n",
    "        yield x,y\n",
    "        if batch_counter == batches:batch_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T07:02:16.625562Z",
     "start_time": "2020-10-20T07:01:54.600362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-14 10:00:24,800\tINFO services.py:1092 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2020-11-14 10:00:24,808\tWARNING services.py:1560 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 16106127360 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'raylet_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-11-14_10-00-23_997436_78/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-11-14_10-00-23_997436_78/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-11-14_10-00-23_997436_78',\n",
       " 'metrics_export_port': 64442,\n",
       " 'node_id': 'ffa0a8c4fb88ea3086ecc99abf10c59bf7485b44'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-20T07:04:32.404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0ae852dc5d45a6965fe12716d0a0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_batches =generate_data_batches(N=96,\n",
    "                                          lR=(0,1),\n",
    "                                          tR=(0,1_000),\n",
    "                                          max_iters=1000,\n",
    "                                          seed=42,\n",
    "                                          batch_size=32,\n",
    "                                          step_size= 10,\n",
    "                                          steps=1,\n",
    "                                          batches=9000,\n",
    "                                          progress=True)\n",
    "#                                           save_file = 'N=100x100 batch_size=64 batches=5000 max_iters=1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-20T07:04:32.404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a064e4dbf44bde8b3ddd58221e45ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_batches_bias =generate_data_batches(N=96,\n",
    "                                          lR=(0,1),\n",
    "                                          tR=(0,1_000),\n",
    "                                          max_iters=20,\n",
    "                                          seed=42,\n",
    "                                          batch_size=32,\n",
    "                                          step_size= 10,\n",
    "                                          steps=1,\n",
    "                                          batches =1_000,\n",
    "                                          key_bias = 9_000,\n",
    "                                          progress = True)\n",
    "#                                           save_file = 'N=100x100 batch_size=64 batches=5000 max_iters=1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T06:28:14.703199Z",
     "start_time": "2020-10-15T06:28:10.596157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021f75462f0840bcad024a6fc873c053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_data_batches =generate_data_batches(N=96,\n",
    "                                              lR=(0,1),\n",
    "                                              tR=(0,1_000),\n",
    "                                              max_iters=1000,\n",
    "                                              seed=0,\n",
    "                                              batch_size=32,\n",
    "                                              step_size= 10,\n",
    "                                              steps=1,\n",
    "                                              batches=50,\n",
    "                                              progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conv(x,f,k):\n",
    "    x = TimeDistributed(Conv2D(f,(k,k),strides=1,padding='same',kernel_initializer='glorot_uniform',activation=LeakyReLU()))(x)\n",
    "    return x\n",
    "    \n",
    "def deconv(x,f,k):\n",
    "    x = TimeDistributed(Conv2DTranspose(f,(k,k),strides=1,padding='same',kernel_initializer='glorot_uniform',activation=LeakyReLU()))(x)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def dense_block(tensor, f, r,k):\n",
    "    for _ in range(r):\n",
    "        x = conv(tensor, f=4*f, k=1)\n",
    "        x = conv(x, f=f, k=k)\n",
    "        tensor = Concatenate()([tensor, x])\n",
    "    return tensor\n",
    "\n",
    "def inv_dense_block(tensor, f, r,k):\n",
    "    for _ in range(r):\n",
    "        x = deconv(tensor, f=4*f, k=1)\n",
    "        x = deconv(x, f=f, k=k)\n",
    "        tensor = Concatenate()([tensor, x])\n",
    "    return tensor\n",
    "\n",
    "def transition(x,s):\n",
    "    ff = int(tensorflow.keras.backend.int_shape(x)[-1] * 0.5)\n",
    "    m0 = TimeDistributed(Conv2D(ff,(1,1),strides=2*s,padding='same',kernel_initializer='glorot_uniform',activation=LeakyReLU()))(x)\n",
    "    return m0\n",
    "\n",
    "def inv_transition(x,s):\n",
    "    ff = int(tensorflow.keras.backend.int_shape(x)[-1] * 0.5)\n",
    "    m0 = TimeDistributed(Conv2DTranspose(ff,(1,1),strides=2*s,padding='same',kernel_initializer='glorot_uniform',activation=LeakyReLU()))(x)\n",
    "    return m0\n",
    "\n",
    "\n",
    "def dfn():\n",
    "    \n",
    "    k=3\n",
    "    s=1;\n",
    "    LR=1e-4\n",
    "    \n",
    "    r1,r2,r3 = 2 , 4 ,8\n",
    "    f0,f1,f2,f3 = 128,32,32,32\n",
    "    l1,l2 = 128 ,64\n",
    "\n",
    "    x = Input(shape=(None, None,None, 1))\n",
    "    c0 = TimeDistributed(Conv2D(f0,(k,k),strides=1,padding='same',kernel_initializer='glorot_uniform',activation=LeakyReLU()))(x)\n",
    "########################################################################################################    \n",
    "    e1 = dense_block(c0,f1,r=r1,k=k);m1 = transition(e1,s)\n",
    "    e2 = dense_block(m1,f2,r=r2,k=k);m2 = transition(e2,s)\n",
    "    e3 = dense_block(m2,f3,r=r3,k=k);\n",
    "########################################################################################################\n",
    "    e = ConvLSTM2D(l1,(2,2),padding='same',return_sequences=True)(e3)\n",
    "    b = ConvLSTM2D(l2,(2,2),padding='same',return_sequences=True)(e)\n",
    "    d = ConvLSTM2D(l1,(2,2),padding='same',return_sequences=True)(b)\n",
    "########################################################################################################\n",
    "    d1 = inv_dense_block(d ,f3,r=r3,k=k);m1 = inv_transition(d1,s)\n",
    "    d2 = inv_dense_block(m1,f2,r=r2,k=k);m2 = inv_transition(d2,s)\n",
    "    d3 = inv_dense_block(m2,f1,r=r1,k=k);\n",
    "########################################################################################################\n",
    "    out = conv(d3,f=1,k=1)\n",
    "    model = Model(x,out)\n",
    "    optimizer = Adam(learning_rate=LR)\n",
    "    model.compile(loss='mae',optimizer=optimizer,metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = dfn()\n",
    "# model.summary()\n",
    "# keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T06:48:10.103493Z",
     "start_time": "2020-10-15T06:48:10.090529Z"
    }
   },
   "outputs": [],
   "source": [
    "history={}\n",
    "i = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 2578s 256ms/step - loss: 0.0726 - mse: 0.0429 - val_loss: 0.0302 - val_mse: 0.0045\n",
      "\n",
      "Epoch 00001: saving model to step=10 96x96.h5\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 2559s 256ms/step - loss: 0.0158 - mse: 0.0022 - val_loss: 0.0309 - val_mse: 0.0041\n",
      "\n",
      "Epoch 00002: saving model to step=10 96x96.h5\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 0.0129 - mse: 0.0017 - val_loss: 0.0313 - val_mse: 0.0043\n",
      "\n",
      "Epoch 00003: saving model to step=10 96x96.h5\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 2558s 256ms/step - loss: 0.0117 - mse: 0.0015 - val_loss: 0.0317 - val_mse: 0.0041\n",
      "\n",
      "Epoch 00004: saving model to step=10 96x96.h5\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 2553s 255ms/step - loss: 0.0109 - mse: 0.0015 - val_loss: 0.0298 - val_mse: 0.0040\n",
      "\n",
      "Epoch 00005: saving model to step=10 96x96.h5\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 0.0106 - mse: 0.0015 - val_loss: 0.0301 - val_mse: 0.0042\n",
      "\n",
      "Epoch 00006: saving model to step=10 96x96.h5\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 2550s 255ms/step - loss: 0.0103 - mse: 0.0014 - val_loss: 0.0298 - val_mse: 0.0039\n",
      "\n",
      "Epoch 00007: saving model to step=10 96x96.h5\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 2553s 255ms/step - loss: 0.0100 - mse: 0.0014 - val_loss: 0.0306 - val_mse: 0.0040\n",
      "\n",
      "Epoch 00008: saving model to step=10 96x96.h5\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 2551s 255ms/step - loss: 0.0098 - mse: 0.0014 - val_loss: 0.0288 - val_mse: 0.0040\n",
      "\n",
      "Epoch 00009: saving model to step=10 96x96.h5\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 2551s 255ms/step - loss: 0.0095 - mse: 0.0013 - val_loss: 0.0287 - val_mse: 0.0039\n",
      "\n",
      "Epoch 00010: saving model to step=10 96x96.h5\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 2551s 255ms/step - loss: 0.0092 - mse: 0.0013 - val_loss: 0.0270 - val_mse: 0.0035\n",
      "\n",
      "Epoch 00011: saving model to step=10 96x96.h5\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 2552s 255ms/step - loss: 0.0088 - mse: 0.0012 - val_loss: 0.0195 - val_mse: 0.0018\n",
      "\n",
      "Epoch 00012: saving model to step=10 96x96.h5\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 2553s 255ms/step - loss: 0.0070 - mse: 8.9647e-04 - val_loss: 0.0095 - val_mse: 7.4839e-04\n",
      "\n",
      "Epoch 00013: saving model to step=10 96x96.h5\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 2555s 255ms/step - loss: 0.0051 - mse: 6.0472e-04 - val_loss: 0.0101 - val_mse: 6.1613e-04\n",
      "\n",
      "Epoch 00014: saving model to step=10 96x96.h5\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 0.0046 - mse: 5.0373e-04 - val_loss: 0.0055 - val_mse: 5.0859e-04\n",
      "\n",
      "Epoch 00015: saving model to step=10 96x96.h5\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 2560s 256ms/step - loss: 0.0044 - mse: 4.8806e-04 - val_loss: 0.0071 - val_mse: 5.2186e-04\n",
      "\n",
      "Epoch 00016: saving model to step=10 96x96.h5\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 2557s 256ms/step - loss: 0.0042 - mse: 4.7766e-04 - val_loss: 0.0052 - val_mse: 4.7530e-04\n",
      "\n",
      "Epoch 00017: saving model to step=10 96x96.h5\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 2557s 256ms/step - loss: 0.0042 - mse: 4.6884e-04 - val_loss: 0.0060 - val_mse: 4.7794e-04\n",
      "\n",
      "Epoch 00018: saving model to step=10 96x96.h5\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 2557s 256ms/step - loss: 0.0040 - mse: 4.5364e-04 - val_loss: 0.0034 - val_mse: 4.0905e-04\n",
      "\n",
      "Epoch 00019: saving model to step=10 96x96.h5\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 2557s 256ms/step - loss: 0.0039 - mse: 3.8188e-04 - val_loss: 0.0091 - val_mse: 3.2373e-04\n",
      "\n",
      "Epoch 00020: saving model to step=10 96x96.h5\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 2558s 256ms/step - loss: 0.0037 - mse: 1.9885e-04 - val_loss: 0.0061 - val_mse: 2.2294e-04\n",
      "\n",
      "Epoch 00021: saving model to step=10 96x96.h5\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 2555s 256ms/step - loss: 0.0036 - mse: 1.7970e-04 - val_loss: 0.0041 - val_mse: 1.7776e-04\n",
      "\n",
      "Epoch 00022: saving model to step=10 96x96.h5\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 0.0035 - mse: 1.7548e-04 - val_loss: 0.0033 - val_mse: 1.7417e-04\n",
      "\n",
      "Epoch 00023: saving model to step=10 96x96.h5\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 2558s 256ms/step - loss: 0.0034 - mse: 1.7246e-04 - val_loss: 0.0041 - val_mse: 1.7231e-04\n",
      "\n",
      "Epoch 00024: saving model to step=10 96x96.h5\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 2555s 255ms/step - loss: 0.0034 - mse: 1.7119e-04 - val_loss: 0.0049 - val_mse: 1.8134e-04\n",
      "\n",
      "Epoch 00025: saving model to step=10 96x96.h5\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 0.0033 - mse: 1.6731e-04 - val_loss: 0.0042 - val_mse: 1.7185e-04\n",
      "\n",
      "Epoch 00026: saving model to step=10 96x96.h5\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 0.0032 - mse: 1.6093e-04 - val_loss: 0.0046 - val_mse: 1.7048e-04\n",
      "\n",
      "Epoch 00027: saving model to step=10 96x96.h5\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 2558s 256ms/step - loss: 0.0033 - mse: 1.4711e-04 - val_loss: 0.0060 - val_mse: 1.4016e-04\n",
      "\n",
      "Epoch 00028: saving model to step=10 96x96.h5\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 2552s 255ms/step - loss: 0.0031 - mse: 6.5718e-05 - val_loss: 0.0051 - val_mse: 5.8266e-05\n",
      "\n",
      "Epoch 00029: saving model to step=10 96x96.h5\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 0.0031 - mse: 2.9617e-05 - val_loss: 0.0045 - val_mse: 4.5510e-05\n",
      "\n",
      "Epoch 00030: saving model to step=10 96x96.h5\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 2548s 255ms/step - loss: 0.0030 - mse: 2.5510e-05 - val_loss: 0.0031 - val_mse: 2.3250e-05\n",
      "\n",
      "Epoch 00031: saving model to step=10 96x96.h5\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 2547s 255ms/step - loss: 0.0029 - mse: 2.3674e-05 - val_loss: 0.0028 - val_mse: 2.0086e-05\n",
      "\n",
      "Epoch 00032: saving model to step=10 96x96.h5\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 2547s 255ms/step - loss: 0.0030 - mse: 2.4333e-05 - val_loss: 0.0048 - val_mse: 4.0421e-05\n",
      "\n",
      "Epoch 00033: saving model to step=10 96x96.h5\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 2547s 255ms/step - loss: 0.0030 - mse: 2.4106e-05 - val_loss: 0.0024 - val_mse: 1.8534e-05\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00034: saving model to step=10 96x96.h5\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 2547s 255ms/step - loss: 0.0015 - mse: 7.2325e-06 - val_loss: 0.0014 - val_mse: 6.9834e-06\n",
      "\n",
      "Epoch 00035: saving model to step=10 96x96.h5\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 2548s 255ms/step - loss: 0.0016 - mse: 7.3469e-06 - val_loss: 0.0015 - val_mse: 6.7306e-06\n",
      "\n",
      "Epoch 00036: saving model to step=10 96x96.h5\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 2551s 255ms/step - loss: 0.0015 - mse: 7.2896e-06 - val_loss: 0.0022 - val_mse: 1.0612e-05\n",
      "\n",
      "Epoch 00037: saving model to step=10 96x96.h5\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 2551s 255ms/step - loss: 0.0015 - mse: 6.9731e-06 - val_loss: 0.0017 - val_mse: 8.0478e-06\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00038: saving model to step=10 96x96.h5\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 8.3512e-04 - mse: 2.8534e-06 - val_loss: 8.9393e-04 - val_mse: 3.6169e-06\n",
      "\n",
      "Epoch 00039: saving model to step=10 96x96.h5\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 8.1623e-04 - mse: 2.6841e-06 - val_loss: 0.0010 - val_mse: 3.9811e-06\n",
      "\n",
      "Epoch 00040: saving model to step=10 96x96.h5\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 2553s 255ms/step - loss: 8.0577e-04 - mse: 2.6396e-06 - val_loss: 9.2539e-04 - val_mse: 3.2689e-06\n",
      "\n",
      "Epoch 00041: saving model to step=10 96x96.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 2555s 256ms/step - loss: 8.3197e-04 - mse: 2.6769e-06 - val_loss: 0.0013 - val_mse: 4.4874e-06\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00042: saving model to step=10 96x96.h5\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 2555s 256ms/step - loss: 5.2677e-04 - mse: 1.6027e-06 - val_loss: 8.7984e-04 - val_mse: 2.8648e-06\n",
      "\n",
      "Epoch 00043: saving model to step=10 96x96.h5\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 2552s 255ms/step - loss: 5.3793e-04 - mse: 1.5702e-06 - val_loss: 6.6652e-04 - val_mse: 2.3888e-06\n",
      "\n",
      "Epoch 00044: saving model to step=10 96x96.h5\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 2555s 256ms/step - loss: 5.1715e-04 - mse: 1.5176e-06 - val_loss: 8.8910e-04 - val_mse: 2.7976e-06\n",
      "\n",
      "Epoch 00045: saving model to step=10 96x96.h5\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 2555s 256ms/step - loss: 5.1620e-04 - mse: 1.4904e-06 - val_loss: 7.6691e-04 - val_mse: 2.4420e-06\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\n",
      "Epoch 00046: saving model to step=10 96x96.h5\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 4.1364e-04 - mse: 1.2535e-06 - val_loss: 5.8422e-04 - val_mse: 1.9831e-06\n",
      "\n",
      "Epoch 00047: saving model to step=10 96x96.h5\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 4.1180e-04 - mse: 1.2229e-06 - val_loss: 5.9858e-04 - val_mse: 1.9732e-06\n",
      "\n",
      "Epoch 00048: saving model to step=10 96x96.h5\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 2563s 256ms/step - loss: 4.0909e-04 - mse: 1.2066e-06 - val_loss: 5.6411e-04 - val_mse: 1.9012e-06\n",
      "\n",
      "Epoch 00049: saving model to step=10 96x96.h5\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 2564s 256ms/step - loss: 4.0028e-04 - mse: 1.1807e-06 - val_loss: 5.4041e-04 - val_mse: 1.8434e-06\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\n",
      "Epoch 00050: saving model to step=10 96x96.h5\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 2564s 256ms/step - loss: 3.6824e-04 - mse: 1.1111e-06 - val_loss: 4.7785e-04 - val_mse: 1.6276e-06\n",
      "\n",
      "Epoch 00051: saving model to step=10 96x96.h5\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 2563s 256ms/step - loss: 3.6382e-04 - mse: 1.0894e-06 - val_loss: 5.1061e-04 - val_mse: 1.6490e-06\n",
      "\n",
      "Epoch 00052: saving model to step=10 96x96.h5\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 2561s 256ms/step - loss: 3.6181e-04 - mse: 1.0782e-06 - val_loss: 4.7485e-04 - val_mse: 1.5935e-06\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\n",
      "Epoch 00053: saving model to step=10 96x96.h5\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 2558s 256ms/step - loss: 3.5047e-04 - mse: 1.0543e-06 - val_loss: 4.4321e-04 - val_mse: 1.4727e-06\n",
      "\n",
      "Epoch 00054: saving model to step=10 96x96.h5\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 2556s 256ms/step - loss: 3.4664e-04 - mse: 1.0388e-06 - val_loss: 4.3961e-04 - val_mse: 1.4607e-06\n",
      "\n",
      "Epoch 00055: saving model to step=10 96x96.h5\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 2554s 255ms/step - loss: 3.4485e-04 - mse: 1.0317e-06 - val_loss: 4.3597e-04 - val_mse: 1.4496e-06\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "\n",
      "Epoch 00056: saving model to step=10 96x96.h5\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf5c1dc780>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_logger = tensorflow.keras.callbacks.CSVLogger('train step=10 96x96.log')\n",
    "early_stopping = tensorflow.keras.callbacks.EarlyStopping(monitor='loss',min_delta=5e-5, patience=5, verbose=1, mode='auto',baseline=None, restore_best_weights=False)\n",
    "reduce_lr_callback = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor = 'loss',factor = 0.5,patience = 3,verbose = 1,cooldown=1,min_delta = 1e-4,min_lr=1e-8 )\n",
    "model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint('step=10 96x96.h5', monitor='loss', verbose=1, save_best_only=False,save_weights_only=False, mode='auto', save_freq='epoch',)\n",
    "model.fit(data_generator({**train_data_batches,**train_data_batches_bias}),\n",
    "          validation_data=data_generator(validation_data_batches),\n",
    "          steps_per_epoch=len({**train_data_batches,**train_data_batches_bias}),\n",
    "          validation_steps=len(validation_data_batches),\n",
    "          verbose=1,\n",
    "          epochs=100,\n",
    "          callbacks=[reduce_lr_callback,early_stopping,csv_logger,model_checkpoint_callback],\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('step=10 96x96 loss=3.4485e-4 4.3597e-4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T14:59:56.465051Z",
     "start_time": "2020-10-17T14:59:55.611505Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T14:59:57.984142Z",
     "start_time": "2020-10-17T14:59:57.671939Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def show_heat_maps(*grids,annotate=False,save=None , rc=None,figsize=(30,10),cbar_location='left'):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if rc is  None : rc=(1,len(grids))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=rc,axes_pad=0.25,share_all=True,cbar_location=cbar_location,cbar_mode=\"single\",cbar_size=\"5%\",cbar_pad=0.25,)\n",
    "    for ax,g in zip(grid,grids):\n",
    "        im = ax.imshow(g[0]) ; \n",
    "        ax.title.set_text(g[1])\n",
    "        if annotate:\n",
    "            N,M = int(g[0].shape[0]),int(g[0].shape[1])\n",
    "            for k in range(N):\n",
    "                for j in range(M):\n",
    "                    text1 = ax.text(j, k, np.round(g[0][k, j],1),ha=\"center\", va=\"center\", color=\"w\",fontsize=20)\n",
    "\n",
    "\n",
    "        ax.cax.colorbar(im)\n",
    "        ax.cax.toggle_label(True)\n",
    "    \n",
    "    if save is not None : plt.savefig(save)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-17T15:03:22.983589Z",
     "start_time": "2020-10-17T15:03:17.236703Z"
    }
   },
   "outputs": [],
   "source": [
    "import  tensorflow\n",
    "loaded_model = model #tensorflow.keras.models.load_model('48x48 loss=6.152e-4 7.5453e-4.h5')\n",
    "loaded_model2 = model # model_scaled #tensorflow.keras.models.load_model('bias 100x100 step=10 loss=0.0012 model 27th october model.h5')\n",
    "\n",
    "# loaded_model2 = keras.models.load_model('100x100 step=10 loss=0.008 model 17th october model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137eaaa0b3a649908bea1f5c8672a8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=600, continuous_update=False, description='bc1 # ', layout=Layout(height…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "layout = ipywidgets.Layout(width= '100%',height='20px')\n",
    "\n",
    "bc1 = ipywidgets.IntSlider(min=0,max=1000,value = 600,step=1,description='bc1 # ' ,layout=layout,continuous_update=False)\n",
    "bc2 = ipywidgets.IntSlider(min=0,max=1000,value=500,step=1,description='bc2 # ' ,layout=layout,continuous_update=False)\n",
    "bc3 = ipywidgets.IntSlider(min=0,max=1000,value=100,step=1,description='bc3 # ' ,layout=layout,continuous_update=False)\n",
    "bc4 = ipywidgets.IntSlider(min=0,max=1000,step=1,value=400,description='bc4 # ' ,layout=layout,continuous_update=False)\n",
    "ic0 = ipywidgets.IntSlider(min=0,max=1000,step=1,value=100,description='ic # ' ,layout=layout,continuous_update=False)\n",
    "lam = ipywidgets.FloatSlider(min=0,max=0.5,value=0.125,step=0.00001,description='lambda # ' ,layout=layout,continuous_update=False,readout_format='.5f')\n",
    "\n",
    "\n",
    "t00 = ipywidgets.IntSlider(min=0,max=10_000,step=1,value=0,description='t00 # ' ,layout=layout,continuous_update=False)\n",
    "t0f = ipywidgets.IntSlider(min=0,max=50_000,step=1,value=t00.value+100,description='t0f # ' ,layout=layout,continuous_update=False)\n",
    "\n",
    "@ipywidgets.interact(bc1=bc1,bc2=bc2,bc3=bc3,bc4=bc4,ic0=ic0,lam=lam,t00=t00,t0f=t0f,analyze=False,plot=False,save=False)\n",
    "def compare_solution(bc1,bc2,bc3,bc4,ic0,lam,t00,t0f,analyze=False,plot=False,save=False):\n",
    "    size = 12  ; mean = 5_00  ;  std  = 2_50 ; step = 10  ; total_step = 1\n",
    "   \n",
    "    '''\n",
    "    generate the adi solution in shape of \n",
    "    (frames , rows ,cols)\n",
    "    '''\n",
    "    tic = time.time()\n",
    "    \n",
    "    grid = generate_grid( size - 2 , bc =(bc1,bc2,bc3,bc4),ic=ic0)\n",
    "    adi_solution = solve(grid.copy(), iters = t00+step*total_step ,Lambda= lam ,steps=True)\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f'Numerical solution excuted in {(toc-tic)*1e3}ms')\n",
    "    \n",
    "    '''\n",
    "    model expects scaled input 5d tensor in shape of \n",
    "    ( sample size , frames number , rows , cols , channels )\n",
    "    standard scaling using mean = 500  ,std = 250\n",
    "    '''\n",
    "    if analyze :\n",
    "        # Preprocessing\n",
    "        model_input = pad_grids(adi_solution,lam)   #pad\n",
    "        model_input = ( adi_solution[t00:t00+1,:,:] - mean ) / std  #scale \n",
    "        model_input = model_input.reshape(1,1,size,size,1)        #reshape to 5d tensor\n",
    "\n",
    "        \n",
    "        plot_list =[]\n",
    "        \n",
    "        prediction_solutions = {}\n",
    "        \n",
    "        tic = time.time() ; \n",
    "        \n",
    "        prediction_solutions[0] = model_input ; \n",
    "        \n",
    "        for i in range(0,total_step+1):\n",
    "            prediction_solutions[i+1] = loaded_model.predict(prediction_solutions[i])\n",
    "        \n",
    "        for i in range(0,total_step+1):\n",
    "            correct    = (adi_solution[t00+step*(i)],f'$Step={t00+step*(i)}$')\n",
    "            prediction = ((prediction_solutions[i][0,0,:,:,0]*std )+mean , f'$Step={t00+step*(i)}$')\n",
    "            plot_list.append(correct)\n",
    "            plot_list.append(prediction)\n",
    "            print(i)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if plot :\n",
    "            if save :path = f'bc:{bc1},{bc2},{bc3},{bc4},{ic0}-lam:{lam}-t00:{t00}.svg' \n",
    "            else: path = None\n",
    "            print('MAE error:',sklearn.metrics.mean_absolute_error(adi_solution[t00+step*(0+1),1:-1,1:-1],prediction_solutions[0][0,0,1:-1,1:-1,0]))\n",
    "            \n",
    "            show_heat_maps(\n",
    "                *plot_list,\n",
    "                annotate=True,\n",
    "                figsize=(50,50),\n",
    "                rc=(len(plot_list)//2,2),\n",
    "                cbar_location='bottom'\n",
    "#             save=path           \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, ''), (1, ''))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =[1,'']\n",
    "b = [2,'']\n",
    "\n",
    "tuple(b),tuple(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow.keras.models.load_model('12 24 48 96 loss=0.0013.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-20T07:04:32.404Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data_batches =generate_data_batches(N=12,\n",
    "                                          lR=(0,1),\n",
    "                                          tR=(0,1_000),\n",
    "                                          max_iters=1000,\n",
    "                                          seed=192,\n",
    "                                          batch_size=1,\n",
    "                                          step_size= 100,\n",
    "                                          steps=1,\n",
    "                                          batches=1000,\n",
    "                                          progress=True)\n",
    "#                                           save_file = 'N=100x100 batch_size=64 batches=5000 max_iters=1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 32s 31ms/step - loss: 0.0372 - mse: 0.0108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03718770667910576, 0.010795393027365208]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data_generator(test_data_batches),steps=len(test_data_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.zeros(1000);std= np.zeros(1000)\n",
    "for bi in test_data_batches:\n",
    "    mean[int(bi)] = np.mean((test_data_batches_12x12[bi] *250 )+ 500) \n",
    "    std[int(bi)]= np.std((test_data_batches_12x12[bi]*250)+500)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500.75358987426756"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
